# -*- coding: utf-8 -*-
"""Maze_110421072.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q6ltr-MOmlBVAOFxWsoN9cYoTjyLxBsU

**利用 RL 逃離迷宮**

**說明:**

迷宮本身為 **21 * 11** 的迷宮

起點左上角 **(0,0)** 終點為右下角 **(20,10)**

- X 表示牆壁位置(顏色為綠色方便區分) 
- O 表示寶藏位置(顏色為橘色方便區分)
- S 表示起點位置
- G 表示終點位置
- 黃色底色表示可能路徑

移動時不可穿過障礙物

**注意事項:**

1. 訓練次數（MAX_EPISODES 參數）不得超過 1000 次

2. 以能到終點為主要作業目標 步數越少分數越高

3. 請自行將規則補在程式內，並制定良好的 Reward 讓你更快走完

4. 寶藏計分方式為參數 Score，每踩到一個寶箱 Score +1，每個寶箱只能獲得一次分數，每個 episode 需重置，且每回合都必須計算 Score 分數，最高 5 分

5. 文件內請附上你認為最好的**一筆**訓練結果（步數 + 寶藏），並將結果保存於 colab 輸出結果中

**牆壁**

陣列位置：

(0,4),(0,5),(0,7),(0,9),(1,1),(1,2),(1,4),(1,9),(1,10),(1,14),(1,18),(2,1),(2,3),(2,5),(2,7),(2,8),(2,9),(2,11),(2,13),(2,15),(2,16),(2,17),(2,19),(3,2),(3,8),(3,11),(3,17),(4,1),(4,4),(4,6),(4,10),(4,13),(4,16),(4,17),(4,18),(4,20),(5,4),(5,5),(5,6),(5,8),(5,9),(5,14),(5,15),(6,1),(6,2),(6,3),(6,6),(6,8),(6,10),(6,15),(6,16),(6,17),(6,19),(7,4),(7,6),(7,8),(7,10),(7,11),(7,17),(7,19),(8,1),(8,4),(8,8),(8,10),(8,13),(8,15),(8,18),(8,19),(9,1),(9,2),(9,4),(9,6),(9,7),(9,17),(10,1),(10,4),(10,16),(10,19)

數值：
4,5,7,9,22,23,25,30,31,35,39,43,45,47,49,50,51,53,55,57,58,59,61,65,71,74,80,85,88,90,94,97,100,101,102,104,109,110,111,113,114,119,120,127,128,129,132,134,136,141,142,143,145,151,153,155,157,158,164,166,169,172,176,178,181,183,186,187,190,191,193,195,196,206,211,214,226,229

**起點**

陣列位置 （0, 0)

數值 0


**終點**

陣列位置 (20, 10)

數值 230


**寶藏**

陣列位置 （0, 6),(3, 16),(8, 2),(10, 2),(10, 17)

數值 6, 79, 170, 212, 227

以下程式碼是以 40 * 30 的空迷宮程式碼

"""

import numpy as np
import pandas as pd
import time

pd.set_option('display.max_rows', 300)

"""設定 x, y 軸長度，

範例為 40 * 30 的無障礙迷宮

並設定可行動動作為上下左右

目標位置在最右下角

以及各項參數設定

SCORE 為計算寶藏分數用 
"""

N_STATES_x=21
N_STATES_y=11
ACTIONS=['left','right','up','down']
GOAL=230
EPSILON=0.9 #探索
ALPHA=0.1
GAMMA=0.9
MAX_EPISODES=500
FRESH_TIME=0

SCORE=0

list_wall = [4,5,7,9,22,23,25,30,31,35,39,43,45,47,49,50,51,53,55,57,58,59,61,65,71,74,80,85,88,90,94,97,100,101,102,104,109,110,111,113,114,119,120,127,128,129,132,134,136,141,142,143,145,151,153,155,157,158,164,166,169,172,176,178,181,183,186,187,190,191,193,195,196,206,211,214,226,229]
list_treasure = [6,79,170,212,227]

"""建立一開始的空Q table"""

def build_q_table(N_STATES_x, N_STATES_y, actions):
    table = pd.DataFrame(
        np.zeros((N_STATES_x * N_STATES_y, len(actions))),
        columns=actions,
    )
    return table

"""如何選擇動作"""

def choose_action(state, q_table):
    state_actions = q_table.iloc[state, :]
    if (np.random.uniform() > EPSILON) or ((state_actions == 0).all()):
        action_name = np.random.choice(ACTIONS)
    else:
        action_name = state_actions.idxmax()
    return action_name

"""Reward 設定
"""

def get_env_feedback(S,A,path):
    if A=='right':
      if S == GOAL:
          S_ = "terminal" 
          R = 30
      elif S % N_STATES_x == N_STATES_x - 1:
          S_ = S 
          R = -10
      elif S+1 in list_wall:
          S_ = S
          R = -10
      elif S+1 =='79':
          S_ = S+1
          R = 6
      else:                  
          S_ = S + 1
          R = -1
      

    if A=='down':
      if S == GOAL:
          S_ = "terminal" 
          R = 30
      elif GOAL - S < N_STATES_x:
          S_ = S 
          R = -10
      elif S+N_STATES_x in list_wall:
          S_ = S
          R = -10
      else: 
          S_ = S + N_STATES_x
          R = -1
      

    if A=='left':
      if S == GOAL:
          S_ = "terminal" 
          R = 30
      elif S % N_STATES_x == 0:
          S_ = S
          R = -10
      elif S-1 in list_wall:
          S_ = S
          R = -10
      elif S - 1 =='227' or S - 1 =='212' or S - 1 =='170':
          S_ = S - 1
          R = 6
      else:      
          S_ = S -1 
          R = -1
      

    if A=='up':
      if S == GOAL: 
          S_ = "terminal" 
          R = 30
      elif S < N_STATES_x:
          S_ = S 
          R = -10
      elif S- N_STATES_x in list_wall:
          S_ = S
          R = -10
      elif S - N_STATES_x =='6':
          S_ = S - N_STATES_x
          R = 6
      else: 
          S_ = S - N_STATES_x
          R = -1

    if S_ in path: #重複走
      R = R - 1

    return S_,R

"""利用選擇的動作更新畫面
"""

def update_env(S, episode, step_counter):
    env_map = [["-" for i in range(N_STATES_x)] for j in range(N_STATES_y)]
    env_map[int(GOAL / N_STATES_x)][GOAL % N_STATES_x] = "T"
    result = []
    if S == "terminal":
        interaction = "Episode %s: total_steps=%s" % (episode + 1, step_counter)
        result.append(interaction)
        print(result)
        print("\r", end="")
    else:
        env_map[int(S / N_STATES_x)][int(S % N_STATES_x)] = "o"
        # time.sleep(FRESH_TIME)

"""# 更新 Q table"""

def rl():
    q_table = build_q_table(N_STATES_x, N_STATES_y, ACTIONS)
    path_all = []
    score_all = []
    shortest_path = []
    max_score = 0

    for episode in range(MAX_EPISODES):
        step_counter = 0
        S = 0
        is_terminated = False
        path = []
        update_env(S, episode, step_counter)

        while not is_terminated:
            A = choose_action(S, q_table)
            path.append(S)
            S_, R = get_env_feedback(S, A, path)
            q_predict = q_table.loc[S, A]

            if S_ != 'terminal':
                q_target = R + GAMMA * q_table.iloc[S_, :].max()
            else:
                q_target = R
                is_terminated = True
                SCORE = 0
                for ele in np.unique(path):
                    if ele in list_treasure:
                        SCORE += 1
                print(SCORE)

            q_table.loc[S, A] += ALPHA * (q_target - q_predict)
            S = S_
            update_env(S, episode, step_counter + 1)
            step_counter += 1

        path_all.append(path)
        score_all.append(SCORE)

    return q_table, path_all, score_all

"""執行程式"""

if __name__ == "__main__":
    q_table, path_all, score_all = rl()
    print("\r\nQ-table:\n")
    print(q_table, path_all, score_all)

print(q_table)
 q_table.to_csv('qtable.csv')

q_table.head(10)

#建立迷宮
maze=[['' for i in range(N_STATES_x)] for j in range(N_STATES_y)]
maze_pd = pd.DataFrame(maze)

for i in range(N_STATES_x*N_STATES_y):
  maze_pd[int(i%N_STATES_x)][int(i/N_STATES_x)]=i

#步數最短

shortest_path_length = min(len(path) for path in path_all)
shortest_path_index = [i for i, path in enumerate(path_all) if len(path) == shortest_path_length][0]

def map_point_shortest(val):
    if val in list_treasure:
        color = 'orange'
    elif val in list_wall:
        color = 'green'
    elif val in path_all[shortest_path_index]:
        color = 'yellow'
    else:
        color = 'black'

    return ('background-color:%s'%color)

print('Score:%s ,Steps:%s'%(score_all[shortest_path_index],len(path_all[shortest_path_index])))
maze_short = maze_pd.style.applymap(map_point_shortest)
maze_short

max_score = max(score_all)
max_score_index = score_all.index(max_score)

def map_point_shortest(val):
    if val in list_treasure:
        color = 'orange'
    elif val in list_wall:
        color = 'green'
    elif val in path_all[max_score_index]:
        color = 'yellow'
    else:
        color = 'black'

    return ('background-color:%s'%color)

print('Score:%s ,Steps:%s'%(score_all[max_score_index],len(path_all[max_score_index])))
maze_short = maze_pd.style.applymap(map_point_shortest)
maze_short
